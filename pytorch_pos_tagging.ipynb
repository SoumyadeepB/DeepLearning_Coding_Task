{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch POS Tagging\n",
    "\n",
    "## Requirements\n",
    "- torch == 1.2.0\n",
    "- torchtext == 0.4.0\n",
    "- tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  1.2.0\n",
      "Torchtext Version:  0.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "print(\"Torch Version: \", torch.__version__)\n",
    "print(\"Torchtext Version: \", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_CACHE = os.path.expanduser(\"~/arbeitsdaten/embeddings/glove/\")\n",
    "DATASET_CACHE = os.path.expanduser(\"./\")\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset is adapted from the UDPOS where the format has been slightly changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTaggingDataset(torchtext.data.TabularDataset):\n",
    "\n",
    "    # Universal Dependencies English Web Treebank by Universal Dependencies contributors\n",
    "    # Modified by Maximilian Schmidt for use at the IMS, University of Stuttgart\n",
    "    # License: http://creativecommons.org/licenses/by-sa/4.0/\n",
    "    urls = ['file:./udpos/en-ud-v2']\n",
    "    dirname = 'en-ud-v2'\n",
    "    name = 'udpos'\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label1_field, label2_field, id_field, root=\".data\", train=\"train.jsonl\",\n",
    "               validation=\"dev.jsonl\",\n",
    "               test=\"test.jsonl\", **kwargs):\n",
    "        \"\"\"Downloads and loads the Universal Dependencies Version 2 POS Tagged\n",
    "        data.\n",
    "        \"\"\"\n",
    "\n",
    "        fields = {'text': text_field}\n",
    "        if label1_field is not None:\n",
    "            fields.update(label1=label1_field)\n",
    "        if label2_field is not None:\n",
    "            fields.update(label2=label2_field)\n",
    "        if id_field is not None:\n",
    "            fields.update(id=id_field)\n",
    "\n",
    "        return super(POSTaggingDataset, cls).splits(\n",
    "            fields=fields, root=root, train=train, validation=validation,\n",
    "            format='json', test=test, **kwargs)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our neural network consists of one fully connected linear layer (and a softmax - see the loss)\n",
    "\n",
    "Embedding layer\n",
    "- maps from indices to vectors\n",
    "- is not trained (freezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    # this resembles a really simple neural network: an embedding layer followed by a fully\n",
    "    # connected linear layer such that predictions are computed for each token in the sequence\n",
    "    # and batch independently\n",
    "    def __init__(self, embedding_vectors, num_classes):\n",
    "        super().__init__()\n",
    "        # Pytorch's embedding layer maps from indices to embeddings, freeze will tell Pytorch to\n",
    "        # not train this layer, i.e. not modifying any weight\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embedding_vectors, freeze=True)\n",
    "        # a fully connected linear layer mapping the embedded vector to a vector of fixed size\n",
    "        # (num_classes in this case)\n",
    "        self.fc = torch.nn.Linear(embedding_vectors.size(1), num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # simple forwarding through our model\n",
    "        # Pytorch takes care of keeping track of the operations for the backward pass\n",
    "        emmedded_inputs = self.embedding(inputs)\n",
    "        outputs = self.fc(emmedded_inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up our fields as placeholder for the actual data\n",
    "\n",
    "- text (input)\n",
    "- label (gold label / ground truth)\n",
    "\n",
    "## Split into training, validation & test dataset and build vocabulary for *training* dataset (only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up fields\n",
    "TEXT = torchtext.data.Field(sequential=True, lower=True, include_lengths=True, batch_first=True, tokenize='spacy')\n",
    "LABEL = torchtext.data.Field(sequential=True, use_vocab=True, batch_first=True, unk_token=None)\n",
    "\n",
    "# make splits for data\n",
    "train, val, test = POSTaggingDataset.splits(root=DATASET_CACHE, text_field=('Text',TEXT), label1_field=None, label2_field=('Label',LABEL), id_field=None)\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300, cache=EMB_CACHE))\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create iterator such that each iteration returns a batch from shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available classes: 51\n",
      "['<pad>', 'NN', 'IN', 'DT', 'NNP', 'PRP', 'JJ', 'RB', '.', 'VB', 'NNS', ',', 'CC', 'VBP', 'VBD', 'VBZ', 'CD', 'VBN', 'VBG', 'MD', 'TO', 'PRP$', '-RRB-', '-LRB-', 'WDT', 'WRB', ':', 'WP', 'UH', '``', \"''\", 'RP', 'HYPH', 'POS', 'NNPS', 'JJR', 'JJS', 'NFP', 'EX', 'ADD', 'GW', 'RBR', '$', 'PDT', 'RBS', 'SYM', 'FW', 'LS', 'AFX', 'WP$', 'XX']\n"
     ]
    }
   ],
   "source": [
    "# make iterator for splits\n",
    "train_iter, val_iter, test_iter = torchtext.data.Iterator.splits((train, val, test), batch_size=BATCH_SIZE, device=DEVICE, sort=False)\n",
    "\n",
    "vocab = TEXT.vocab\n",
    "classes = LABEL.vocab.itos\n",
    "print(f\"Available classes: {len(classes)}\\n{classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model and optimizer\n",
    "- Cross Entropy is Softmax + Negative Log Likelihood\n",
    "- Optimizer is Stochastic Gradient Descent (with momentum)\n",
    "\n",
    "(run this only once as Jupyter keeps the model (including the weights) and the optimizer in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model and optimizer\n",
    "model = Net(vocab.vectors, len(classes))\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "metric_dict = {'loss': '------', 'accuracy': '------'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function comparing prediction with gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_iter, net):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # extract input and labels\n",
    "        (inputs, inputs_lengths), labels = batch.Text, batch.Label\n",
    "\n",
    "        # predict only\n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs).cpu()\n",
    "        outputs_classes = outputs.argmax(dim=2)\n",
    "\n",
    "        # compute amount of correct predictions\n",
    "        # sequence lengths within the batch might be different, so we need to take care of that\n",
    "\n",
    "        total_count += inputs_lengths.sum()\n",
    "        # option 1: iterate over each sample of the batch\n",
    "        batch_size = outputs_classes.size(0)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(inputs_lengths[i]):\n",
    "                correct_count += int(outputs_classes[i][j] == labels[i][j])\n",
    "    return correct_count/total_count.float().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual training loop\n",
    "\n",
    "- runs several epochs\n",
    "- in each epoch\n",
    " - forward the batch\n",
    " - computes the loss for the output of the whole batch\n",
    " - reduces (e.g. average, sum) the loss\n",
    " - computes derivatives of weights by backpropagation\n",
    " - optimizer updates weights\n",
    " - evaluate on validation/development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdd3c97c4214564b7c467884a698b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=116360), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "# a nice progress bar to make the waiting time much better\n",
    "pbar = tqdm(total=NUM_EPOCHS*len(train), postfix=metric_dict)\n",
    "\n",
    "# run for NUM_EPOCHS epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # run for every data (in batches) of our iterator\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        # extract input and labels\n",
    "        (inputs, inputs_lengths), labels = batch.Text, batch.Label\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 2D loss function expects input as (batch, prediction, sequence) and target as (batch, sequence) (containing the class INDEX)\n",
    "        # loss = criterion(outputs.permute(0,2,1), labels)\n",
    "        # otherwise use view function to get rid of sequence dimension by effectively concatenating all sequence items\n",
    "        loss = criterion(outputs.view(-1, len(classes)), labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        pbar.update(labels.size(0))\n",
    "        metric_dict.update({'loss': f'{loss.item():6.3f}'})\n",
    "        pbar.set_postfix(metric_dict)\n",
    "        \n",
    "    # evaluate on validation set after each epoch\n",
    "    metric_dict.update({'accuracy': f'{100*evaluate(val_iter, model):6.2f}%'})\n",
    "    pbar.set_postfix(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_index(tokens: list, vocabulary: dict):\n",
    "    return [vocabulary[token] for token in tokens]\n",
    "\n",
    "def indices_to_class(indices: list, classes: dict):\n",
    "    return [classes[value] for value in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get any tokenizer\n",
    "tokenizer = torchtext.data.get_tokenizer('spacy', language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your text: How is the weather tomorrow?\n",
      "Prediction:  ['<pad>', 'VBZ', 'DT', 'NN', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Please enter your text: \")\n",
    "\n",
    "# map tokens to index using vocabulary\n",
    "tokens = tokenizer(text)\n",
    "tokens_indexed = tokens_to_index(tokens, vocab)\n",
    "# build input vector and add batch dimension\n",
    "tensor = torch.tensor(tokens_indexed).unsqueeze(dim=0)\n",
    "\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    # get rid of batch dimension (is set to 1)\n",
    "    outputs = model(tensor).squeeze(dim=0)\n",
    "\n",
    "print(\"Prediction: \", indices_to_class(outputs.argmax(dim=1), classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly predict sample from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\t\t     the staff is also just so pleasant to deal with .\n",
      "Prediction:\t     ['DT', 'NN', 'VBZ', 'IN', 'RB', 'RB', 'JJ', 'TO', 'NN', 'IN', '.']\n",
      "Expected prediction: ['DT', 'NN', 'VBZ', 'RB', 'RB', 'RB', 'JJ', 'TO', 'VB', 'IN', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_idx = random.randint(1, len(test))\n",
    "sample = test[sample_idx]\n",
    "# map tokens to index using vocabulary\n",
    "sample_tokens_indexed = tokens_to_index(sample.Text, vocab)\n",
    "# build input vector and add batch dimension\n",
    "sample_tensor = torch.tensor(sample_tokens_indexed).unsqueeze(dim=0)\n",
    "\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    # get rid of batch dimension (is set to 1)\n",
    "    outputs = model(sample_tensor).squeeze(dim=0)\n",
    "\n",
    "print(\"Input:\\t\\t    \", ' '.join(sample.Text))\n",
    "print(\"Prediction:\\t    \", indices_to_class(outputs.argmax(dim=1), classes))\n",
    "print(\"Expected prediction:\", sample.Label)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
